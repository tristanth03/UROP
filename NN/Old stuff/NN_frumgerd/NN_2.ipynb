{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: A has dimensions (784,60000) but B has dimensions (784,60000)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: A has dimensions (784,60000) but B has dimensions (784,60000)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] gemm_wrapper!(C::Matrix{Float32}, tA::Char, tB::Char, A::Matrix{Float32}, B::Matrix{Float32}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})\n",
      "    @ LinearAlgebra C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:646\n",
      "  [2] mul!\n",
      "    @ C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:161 [inlined]\n",
      "  [3] mul!\n",
      "    @ C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:276 [inlined]\n",
      "  [4] *\n",
      "    @ C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:148 [inlined]\n",
      "  [5] rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ChainRules\\pEOSw\\src\\rulesets\\Base\\arraymath.jl:64 [inlined]\n",
      "  [6] rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ChainRulesCore\\7MWx2\\src\\rules.jl:134 [inlined]\n",
      "  [7] chain_rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\chainrules.jl:223 [inlined]\n",
      "  [8] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:101 [inlined]\n",
      "  [9] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:101 [inlined]\n",
      " [10] _pullback\n",
      "    @ c:\\UROP\\NN_frumgerd\\NN_2.ipynb:91 [inlined]\n",
      " [11] _pullback(ctx::Zygote.Context{true}, f::typeof(model_5_3136), args::Matrix{Float32})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:0\n",
      " [12] _pullback\n",
      "    @ c:\\UROP\\NN_frumgerd\\NN_2.ipynb:120 [inlined]\n",
      " [13] _pullback(::Zygote.Context{true}, ::var\"#loss_5_3136#12\"{typeof(model_5_3136)}, ::Matrix{Float32}, ::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:0\n",
      " [14] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [15] adjoint\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\lib\\lib.jl:203 [inlined]\n",
      " [16] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ZygoteRules\\4nXuu\\src\\adjoint.jl:66 [inlined]\n",
      " [17] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:92 [inlined]\n",
      " [18] _pullback(::Zygote.Context{true}, ::Flux.Optimise.var\"#37#40\"{var\"#loss_5_3136#12\"{typeof(model_5_3136)}, Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:0\n",
      " [19] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface.jl:414\n",
      " [20] withgradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface.jl:154\n",
      " [21] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:91 [inlined]\n",
      " [22] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ProgressLogging\\6KXlp\\src\\ProgressLogging.jl:328 [inlined]\n",
      " [23] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Vector{Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}}, opt::Descent; cb::Flux.Optimise.var\"#38#41\")\n",
      "    @ Flux.Optimise C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:90\n",
      " [24] train!\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:86 [inlined]\n",
      " [25] train_batch(X_train::Matrix{Float32}, Y_train::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, loss::Function, model::Chain{NTuple{5, Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}}}, opt::Descent, params::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, epochs::Int64)\n",
      "    @ Main c:\\UROP\\NN_frumgerd\\NN_2.ipynb:133\n",
      " [26] top-level scope\n",
      "    @ c:\\UROP\\NN_frumgerd\\NN_2.ipynb:155"
     ]
    }
   ],
   "source": [
    "#---- Packages\n",
    "\n",
    "# Assuming that packages are already installed:\n",
    "# using Pkg\n",
    "# Pkg.add(\"Images\") ...\n",
    "\n",
    "using Images\n",
    "using FileIO\n",
    "using MLDatasets\n",
    "using Flux\n",
    "using ImageShow\n",
    "using ImageInTerminal\n",
    "using ImageIO\n",
    "using ImageMagick\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "\n",
    "#---- Functions\n",
    "# function img_to_FloatVector(img_path,scale)\n",
    "#     '''\n",
    "#     '''\n",
    "    \n",
    "#     img = load(img_path)\n",
    "\n",
    "#     gray_img = Gray.(img)\n",
    "#     resized_img = imresize(gray_img, (scale,scale))\n",
    "#     single_img = 1 .-float.(resized_img)\n",
    "#     single_data = 1 .-reshape(Float32.(resized_img),(scale*scale,1))\n",
    "\n",
    "#     return single_img, single_data\n",
    "# end\n",
    "\n",
    "function load_MNIST()\n",
    "    \"\"\"\n",
    "    Loading the MNIST dataset.\n",
    "    X: Grayscale vector, Y: Label (assuming that the label is correct).\n",
    "    \"\"\"\n",
    "    \n",
    "    X_training, Y_training = MNIST(split = :train)[:] \n",
    "    X_testing, Y_testing = MNIST(split = :test)[:]\n",
    "\n",
    "    X_training = Flux.flatten(X_training)\n",
    "    X_testing = Flux.flatten(X_testing)\n",
    "    Y_training = Flux.onehotbatch(Y_training,0:9)\n",
    "    Y_testing = Flux.onehotbatch(Y_testing,0:9)\n",
    "\n",
    "    return X_training,Y_training,X_testing,Y_testing\n",
    "\n",
    "end\n",
    "\n",
    "function model_4LS(scale)\n",
    "    \"\"\"\n",
    "    4LS: A 4-layer model using 16 nodes in the inner layers and the sigmoid activation function\n",
    "    \"\"\"\n",
    "\n",
    "    m_4LS = Chain(\n",
    "        Dense(scale*scale,16,sigmoid), # Input Layer -> Hidden Layer 1\n",
    "        Dense(16,16,sigmoid), # Hidden Layer 1 -> Hidden Layer 2\n",
    "        Dense(16,16,sigmoid), # Hidden Layer 2 -> Hidden Layer 3\n",
    "        Dense(16,10,sigmoid) # Hidden Layer 3 -> Output Layer\n",
    "        )\n",
    "\n",
    "    params_4LS = Flux.params(m_4LS) # The parameters\n",
    "    \n",
    "    return m_4LS,params_4LS\n",
    "end\n",
    "\n",
    "function model_3LS(scale)\n",
    "    \"\"\"\n",
    "    A 3-layer model using 60 nodes in the inner layers and the sigmoid activation function\n",
    "    \"\"\"\n",
    "\n",
    "    m_3LS = Chain(\n",
    "        Dense(scale*scale,60,sigmoid), # Input Layer -> Hidden Layer 1\n",
    "        Dense(60,60,sigmoid), # Hidden Layer 1 -> Hidden Layer 2\n",
    "        Dense(60,10,sigmoid) # Hidden Layer 2 -> Output Layer\n",
    "        \n",
    "    )\n",
    "\n",
    "    param_3LS = Flux.params(m_3LS) # The parameters\n",
    "\n",
    "    return m_3LS,param_3LS\n",
    "end\n",
    "\n",
    "\n",
    "function model_5_3136(scale)\n",
    "    \"\"\"\n",
    "    A 5-layer model using 3136 nodes in the hidden layers, sigmoid activation function\n",
    "    \"\"\"\n",
    "    m_5_3136 = Chain(\n",
    "        Dense(scale*scale,3136,sigmoid), # Input layer -> Hidden\n",
    "        Dense(3136,3136,sigmoid), \n",
    "        Dense(3136,3136,sigmoid),\n",
    "        Dense(3136,3136,sigmoid),\n",
    "        Dense(3136,10,sigmoid)\n",
    "    )\n",
    "\n",
    "    param_5_3136 = Flux.params(m_5_3136)\n",
    "    return m_5_3136,param_5_3136\n",
    "end\n",
    "\n",
    "function loss_of(model_3LS)\n",
    "    \"\"\"\n",
    "    For a loss function we use MSE(mean squared error)\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    loss_3LS(X_LS3,Y_LS3) =  Flux.Losses.mse(model_3LS(X_LS3),Y_LS3) \n",
    "\n",
    "    return loss_3LS\n",
    "\n",
    "end\n",
    "\n",
    "function loss_of(model_5_3136)\n",
    "    \"\"\"\n",
    "    For the loss of m_5_3136\n",
    "    \"\"\"\n",
    "\n",
    "    loss_5_3136(X_5_3136,Y_5_3136) = Flux.Losses.mse(model_5_3136(X_5_3136),Y_5_3136)\n",
    "\n",
    "    return loss_5_3136\n",
    "end\n",
    "\n",
    "function train_batch(X_train, Y_train, loss, model, opt, params, epochs)\n",
    "    \"\"\"\n",
    "    In: data, loss, optimizer, parameters, iteration(epochs)\n",
    "    Out: trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    data = [(X_train, Y_train)]\n",
    "    for epoch in 1:epochs\n",
    "        Flux.train!(loss, params, data, opt)\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0 \n",
    "            println(\"Epoch $epoch of $epochs completed.\")\n",
    "            \n",
    "        end\n",
    "        \n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Data\n",
    "X_training,Y_training,X_testing,Y_testing = load_MNIST()\n",
    "\n",
    "scale = 28 # 28x28 scale\n",
    "\n",
    "\n",
    "m_5_3136,params_5_3136 = model_5_3136(scale)\n",
    "loss_5_3136 = loss_of(model_5_3136)\n",
    "epochs = 1000 # Iterations\n",
    "lr = 0.1 # Learning rate\n",
    "training_5_3136 = train_batch(X_training,Y_training,loss_5_3136,m_5_3136,Descent(lr),params_5_3136,epochs)\n",
    "loss_update_5_3136 = loss_5_3136(X_training,Y_training)\n",
    "println(\"Loss update: $loss_update_5_3136\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: A has dimensions (784,60000) but B has dimensions (784,60000)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: A has dimensions (784,60000) but B has dimensions (784,60000)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] gemm_wrapper!(C::Matrix{Float32}, tA::Char, tB::Char, A::Matrix{Float32}, B::Matrix{Float32}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})\n",
      "    @ LinearAlgebra C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:646\n",
      "  [2] mul!\n",
      "    @ C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:161 [inlined]\n",
      "  [3] mul!\n",
      "    @ C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:276 [inlined]\n",
      "  [4] *\n",
      "    @ C:\\Programming\\Julia-1.9.4\\share\\julia\\stdlib\\v1.9\\LinearAlgebra\\src\\matmul.jl:148 [inlined]\n",
      "  [5] rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ChainRules\\pEOSw\\src\\rulesets\\Base\\arraymath.jl:64 [inlined]\n",
      "  [6] rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ChainRulesCore\\7MWx2\\src\\rules.jl:134 [inlined]\n",
      "  [7] chain_rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\chainrules.jl:223 [inlined]\n",
      "  [8] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:101 [inlined]\n",
      "  [9] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:101 [inlined]\n",
      " [10] _pullback\n",
      "    @ c:\\UROP\\NN_frumgerd\\NN_2.ipynb:23 [inlined]\n",
      " [11] _pullback(ctx::Zygote.Context{true}, f::typeof(model_5_3136), args::Matrix{Float32})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:0\n",
      " [12] _pullback\n",
      "    @ c:\\UROP\\NN_frumgerd\\NN_2.ipynb:60 [inlined]\n",
      " [13] _pullback(::Zygote.Context{true}, ::var\"#loss_5_3136#20\"{typeof(model_5_3136)}, ::Matrix{Float32}, ::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:0\n",
      " [14] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [15] adjoint\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\lib\\lib.jl:203 [inlined]\n",
      " [16] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ZygoteRules\\4nXuu\\src\\adjoint.jl:66 [inlined]\n",
      " [17] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:92 [inlined]\n",
      " [18] _pullback(::Zygote.Context{true}, ::Flux.Optimise.var\"#37#40\"{var\"#loss_5_3136#20\"{typeof(model_5_3136)}, Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface2.jl:0\n",
      " [19] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface.jl:414\n",
      " [20] withgradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\YYT6v\\src\\compiler\\interface.jl:154\n",
      " [21] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:91 [inlined]\n",
      " [22] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ProgressLogging\\6KXlp\\src\\ProgressLogging.jl:328 [inlined]\n",
      " [23] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Vector{Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}}, opt::Descent; cb::Flux.Optimise.var\"#38#41\")\n",
      "    @ Flux.Optimise C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:90\n",
      " [24] train!\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\optimise\\train.jl:86 [inlined]\n",
      " [25] train_batch(X_train::Matrix{Float32}, Y_train::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, loss::Function, model::Chain{NTuple{5, Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}}}, opt::Descent, params::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, epochs::Int64)\n",
      "    @ Main c:\\UROP\\NN_frumgerd\\NN_2.ipynb:43\n",
      " [26] top-level scope\n",
      "    @ c:\\UROP\\NN_frumgerd\\NN_2.ipynb:72"
     ]
    }
   ],
   "source": [
    "function load_MNIST()\n",
    "    \"\"\"\n",
    "    Loading the MNIST dataset.\n",
    "    X: Grayscale vector, Y: Label (assuming that the label is correct).\n",
    "    \"\"\"\n",
    "    \n",
    "    X_training, Y_training = MNIST(split = :train)[:] \n",
    "    X_testing, Y_testing = MNIST(split = :test)[:]\n",
    "\n",
    "    X_training = Flux.flatten(X_training)\n",
    "    X_testing = Flux.flatten(X_testing)\n",
    "    Y_training = Flux.onehotbatch(Y_training,0:9)\n",
    "    Y_testing = Flux.onehotbatch(Y_testing,0:9)\n",
    "\n",
    "    return X_training,Y_training,X_testing,Y_testing\n",
    "\n",
    "end\n",
    "\n",
    "function model_5_3136(scale)\n",
    "    \"\"\"\n",
    "    A 5-layer model using 3136 nodes in the hidden layers, sigmoid activation function\n",
    "    \"\"\"\n",
    "    m_5_3136 = Chain(\n",
    "        Dense(scale*scale,3136,sigmoid), # Input layer -> Hidden\n",
    "        Dense(3136,3136,sigmoid), \n",
    "        Dense(3136,3136,sigmoid),\n",
    "        Dense(3136,3136,sigmoid),\n",
    "        Dense(3136,10,sigmoid)\n",
    "    )\n",
    "\n",
    "    param_5_3136 = Flux.params(m_5_3136)\n",
    "    return m_5_3136,param_5_3136\n",
    "end\n",
    "\n",
    "function train_batch(X_train, Y_train, loss, model, opt, params, epochs)\n",
    "    \"\"\"\n",
    "    In: data, loss, optimizer, parameters, iteration(epochs)\n",
    "    Out: trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    data = [(X_train, Y_train)]\n",
    "    for epoch in 1:epochs\n",
    "        Flux.train!(loss, params, data, opt)\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0 \n",
    "            println(\"Epoch $epoch of $epochs completed.\")\n",
    "            \n",
    "        end\n",
    "        \n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function loss_of(model_5_3136)\n",
    "    \"\"\"\n",
    "    For the loss of m_5_3136\n",
    "    \"\"\"\n",
    "\n",
    "    loss_5_3136(X_5_3136,Y_5_3136) = Flux.Losses.mse(model_5_3136(X_5_3136),Y_5_3136)\n",
    "\n",
    "    return loss_5_3136\n",
    "end\n",
    "\n",
    "\n",
    "X_training,Y_training,X_testing,Y_testing = load_MNIST()\n",
    "scale = 28 # 28x28 scale\n",
    "m_5_3136,params_5_3136 = model_5_3136(scale)\n",
    "loss_5_3136 = loss_of(model_5_3136)\n",
    "epochs = 100 # Iterations\n",
    "lr = 0.1 # Learning rate\n",
    "training_5_3136 = train_batch(X_training,Y_training,loss_5_3136,m_5_3136,Descent(lr),params_5_3136,epochs)\n",
    "loss_update_5_3136 = loss_5_3136(X_training,Y_training)\n",
    "println(\"Loss update: $loss_update_5_3136\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Packages\n",
    "\n",
    "# Assuming that packages are already installed:\n",
    "# using Pkg\n",
    "# Pkg.add(\"Images\") ...\n",
    "\n",
    "using Images\n",
    "using FileIO\n",
    "using MLDatasets\n",
    "using Flux\n",
    "using ImageShow\n",
    "using ImageInTerminal\n",
    "using ImageIO\n",
    "using ImageMagick\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "\n",
    "#---- Functions\n",
    "# function img_to_FloatVector(img_path,scale)\n",
    "#     '''\n",
    "#     '''\n",
    "    \n",
    "#     img = load(img_path)\n",
    "\n",
    "#     gray_img = Gray.(img)\n",
    "#     resized_img = imresize(gray_img, (scale,scale))\n",
    "#     single_img = 1 .-float.(resized_img)\n",
    "#     single_data = 1 .-reshape(Float32.(resized_img),(scale*scale,1))\n",
    "\n",
    "#     return single_img, single_data\n",
    "# end\n",
    "\n",
    "function load_MNIST()\n",
    "    \"\"\"\n",
    "    Loading the MNIST dataset.\n",
    "    X: Grayscale vector, Y: Label (assuming that the label is correct).\n",
    "    \"\"\"\n",
    "    \n",
    "    X_training, Y_training = MNIST(split = :train)[:] \n",
    "    X_testing, Y_testing = MNIST(split = :test)[:]\n",
    "\n",
    "    X_training = Flux.flatten(X_training)\n",
    "    X_testing = Flux.flatten(X_testing)\n",
    "    Y_training = Flux.onehotbatch(Y_training,0:9)\n",
    "    Y_testing = Flux.onehotbatch(Y_testing,0:9)\n",
    "\n",
    "    return X_training,Y_training,X_testing,Y_testing\n",
    "\n",
    "end\n",
    "\n",
    "function model_4LS(scale)\n",
    "    \"\"\"\n",
    "    4LS: A 4-layer model using 16 nodes in the inner layers and the sigmoid activation function\n",
    "    \"\"\"\n",
    "\n",
    "    m_4LS = Chain(\n",
    "        Dense(scale*scale,16,sigmoid), # Input Layer -> Hidden Layer 1\n",
    "        Dense(16,16,sigmoid), # Hidden Layer 1 -> Hidden Layer 2\n",
    "        Dense(16,16,sigmoid), # Hidden Layer 2 -> Hidden Layer 3\n",
    "        Dense(16,10,sigmoid) # Hidden Layer 3 -> Output Layer\n",
    "        )\n",
    "\n",
    "    params_4LS = Flux.params(m_4LS) # The parameters\n",
    "    \n",
    "    return m_4LS,params_4LS\n",
    "end\n",
    "\n",
    "function model_3LS(scale)\n",
    "    \"\"\"\n",
    "    A 3-layer model using 60 nodes in the inner layers and the sigmoid activation function\n",
    "    \"\"\"\n",
    "\n",
    "    m_3LS = Chain(\n",
    "        Dense(scale*scale,3136,sigmoid), # Input layer -> Hidden\n",
    "        Dense(3136,3136,sigmoid), \n",
    "        Dense(3136,3136,sigmoid),\n",
    "        Dense(3136,3136,sigmoid),\n",
    "        Dense(3136,10,sigmoid)\n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "    param_3LS = Flux.params(m_3LS) # The parameters\n",
    "\n",
    "    return m_3LS,param_3LS\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function loss_of(model_3LS)\n",
    "    \"\"\"\n",
    "    For a loss function we use MSE(mean squared error)\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    loss_3LS(X_LS3,Y_LS3) =  Flux.Losses.mse(model_3LS(X_LS3),Y_LS3) \n",
    "\n",
    "    return loss_3LS\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function train_batch(X_train, Y_train, loss, model, opt, params, epochs)\n",
    "    \"\"\"\n",
    "    In: data, loss, optimizer, parameters, iteration(epochs)\n",
    "    Out: trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    data = [(X_train, Y_train)]\n",
    "    for epoch in 1:epochs\n",
    "        Flux.train!(loss, params, data, opt)\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0 \n",
    "            println(\"Epoch $epoch of $epochs completed.\")\n",
    "            \n",
    "        end\n",
    "        \n",
    "    end\n",
    "end\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---- Main\n",
    "\n",
    "# Data\n",
    "X_training,Y_training,X_testing,Y_testing = load_MNIST()\n",
    "\n",
    "scale = 28 # 28x28 scale\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "m_3LS,params_3LS = model_3LS(scale)\n",
    "\n",
    "# Loss\n",
    "loss_3LS = loss_of(m_3LS)\n",
    "\n",
    "# m_5_3136,params_5_3136 = model_5_3136(scale)\n",
    "# loss_5_3136 = loss_of(m_5_3136)\n",
    "\n",
    "# Training\n",
    "epochs = 1 # Iterations\n",
    "lr = 0.1 # Learning rate\n",
    "\n",
    "\n",
    "training_3LS = train_batch(X_training, Y_training,loss_3LS, m_3LS, Descent(lr), params_3LS, epochs)\n",
    "\n",
    "# Loss update\n",
    "\n",
    "loss_update_3LS = loss_3LS(X_training,Y_training)\n",
    "println(\"Loss update: $loss_update_3LS\")\n",
    "\n",
    "# training_5_3136 = train_batch(X_training,Y_training,loss_5_3136,m_5_3136,Descent(lr),params_5_3136,epochs)\n",
    "\n",
    "# Testing\n",
    "\n",
    "# function predict_digit(model, single_data)\n",
    "#     \"\"\"\n",
    "#     Predicts the digit from the given image data using the specified model.\n",
    "#     Ensures that the data is in the correct format and handles the model's output appropriately.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Ensure single_data is a column vector as expected by the model\n",
    "#     if size(single_data, 1) != scale*scale || size(single_data, 2) != 1\n",
    "#         error(\"Input data must be a column vector of size $(scale*scale).\")\n",
    "#     end\n",
    "\n",
    "#     # Forward pass through the model\n",
    "#     prediction = model(single_data)\n",
    "\n",
    "#     # Ensure prediction is a vector\n",
    "#     if !(prediction isa AbstractVector) || length(prediction) != 10\n",
    "#         error(\"Model output must be a vector of length 10.\")\n",
    "#     end\n",
    "\n",
    "#     # Convert to probability scores if not already\n",
    "#     if sum(prediction) ≈ 1\n",
    "#         prob_scores = prediction\n",
    "#     else\n",
    "#         prob_scores = softmax(prediction)\n",
    "#     end\n",
    "\n",
    "#     # Find the index of the maximum value in the prediction\n",
    "#     digit = argmax(prob_scores) - 1 # Adjusting for Julia's 1-indexing\n",
    "\n",
    "#     return digit\n",
    "# end\n",
    "\n",
    "# digit = Y_testing[:,1]\n",
    "# single_data_fixed = X_testing[:,1]\n",
    "# predicted_digit = predict_digit(m_3LS, single_data_fixed)\n",
    "# println(\"Predicted digit: $predicted_digit, Digit: $digit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
