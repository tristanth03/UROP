{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors: Axel & Tristan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux,CairoMakie,Zygote,LinearAlgebra,IterTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm_params (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function node_count(model)\n",
    "    \"\"\"Function checks how many nodes are in each layer\n",
    "       Including  the input, ouput and hidden layers\"\"\"\n",
    "    param(x)  = Flux.params(model)[x]\n",
    "    n = []\n",
    "    for i = 1:length(Flux.params(model))\n",
    "        if i%2 != 0 # Check all weights in θ \n",
    "            ni = size(param(i))[2]\n",
    "            push!(n,ni)                 \n",
    "        end                             \n",
    "    end\n",
    "\n",
    "    \"\"\"We also have to check the output layer specifically.  \n",
    "       This is because there is not weight in θ associated with the output layer\"\"\"\n",
    "    push!(n, length(param(length(Flux.params(model)))))\n",
    "    return n\n",
    "end\n",
    "\n",
    "\n",
    "function norm_params(model)\n",
    "    θ(x)  = Flux.params(model)[x]\n",
    "    nNodes = node_count(model)\n",
    "\n",
    "    i = 1\n",
    "    for n = 1:length(nNodes)-1\n",
    "        ni = nNodes[n]\n",
    "\n",
    "        θ(i) .= θ(i) * 1/sqrt(ni)\n",
    "        i += 1\n",
    "        θ(i) .= θ(i) * 1/sqrt(ni)\n",
    "        i += 1\n",
    "\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = [0.4]\n",
    "# x2 = [1.1]\n",
    "# n1 = 10\n",
    "\n",
    "\n",
    "# model = Chain(Dense(1,n1,sigmoid),Dense(n1,1))|>f64\n",
    "\n",
    "\n",
    "\n",
    "# norm_params(model)\n",
    "\n",
    "# ∇_x1 = gradient(()->model(x1)[1],Flux.params(model))\n",
    "# ∇_x2 = gradient(()->model(x2)[1],Flux.params(model))\n",
    "\n",
    "# n = 2\n",
    "# m = 2\n",
    "# K = zeros(m,n)\n",
    "# K[1,1] = dot(∇_x1,∇_x1)\n",
    "# K[2,2] = dot(∇_x2,∇_x2)\n",
    "# K[1,2] = dot(∇_x1,∇_x2)\n",
    "# K[2,1] = dot(∇_x2,∇_x1)\n",
    "\n",
    "# K\n",
    "\n",
    "# λ = eigen(K).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: layer Dense(1 => 1000, σ) expects size(input, 1) == 1, but got 21-element Vector{Float64}",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: layer Dense(1 => 1000, σ) expects size(input, 1) == 1, but got 21-element Vector{Float64}\n",
      "\n",
      "Stacktrace:\n",
      "  [1] _size_check(layer::Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, x::Vector{Float64}, ::Pair{Int64, Int64})\n",
      "    @ Flux C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\layers\\basic.jl:195\n",
      "  [2] rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\layers\\basic.jl:198 [inlined]\n",
      "  [3] rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\ChainRulesCore\\7MWx2\\src\\rules.jl:134 [inlined]\n",
      "  [4] chain_rrule\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\chainrules.jl:223 [inlined]\n",
      "  [5] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface2.jl:101 [inlined]\n",
      "  [6] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface2.jl:101 [inlined]\n",
      "  [7] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\layers\\basic.jl:171 [inlined]\n",
      "  [8] _pullback(ctx::Zygote.Context{true}, f::Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, args::Vector{Float64})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface2.jl:0\n",
      "  [9] macro expansion\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\layers\\basic.jl:53 [inlined]\n",
      " [10] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\layers\\basic.jl:53 [inlined]\n",
      " [11] _pullback(::Zygote.Context{true}, ::typeof(Flux._applychain), ::Tuple{Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, ::Vector{Float64})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface2.jl:0\n",
      " [12] _pullback\n",
      "    @ C:\\Users\\trist\\.julia\\packages\\Flux\\jgpVj\\src\\layers\\basic.jl:51 [inlined]\n",
      " [13] _pullback(ctx::Zygote.Context{true}, f::Chain{Tuple{Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, args::Vector{Float64})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface2.jl:0\n",
      " [14] _pullback\n",
      "    @ c:\\Programming\\Github\\UROP\\Phase3\\Trist.ipynb:17 [inlined]\n",
      " [15] _pullback(::Zygote.Context{true}, ::var\"#13#14\"{Int64})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface2.jl:0\n",
      " [16] pullback(f::Function, ps::Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface.jl:414\n",
      " [17] gradient(f::Function, args::Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote C:\\Users\\trist\\.julia\\packages\\Zygote\\WOy6z\\src\\compiler\\interface.jl:96\n",
      " [18] top-level scope\n",
      "    @ c:\\Programming\\Github\\UROP\\Phase3\\Trist.ipynb:17"
     ]
    }
   ],
   "source": [
    "x1 = [0.4]\n",
    "x2 = [1.1]\n",
    "n1 = 1000\n",
    "\n",
    "\n",
    "model = Chain(Dense(1,n1,sigmoid),Dense(n1,1))\n",
    "norm_params(model)\n",
    "\n",
    "\n",
    "xR = range(-1,1,step=0.1)\n",
    "xR2 = range(-1,1,step=0.1)\n",
    "xn = hcat(xR,xR2)\n",
    "∇_xn = []\n",
    "Q = length(model)\n",
    "N = length(xn)\n",
    "for xi = 1:N\n",
    "    ∇_i = gradient(()->model(xn[:,xi])[1],Flux.params(model))\n",
    "    push!(∇_xn, ∇_i)\n",
    "end\n",
    "\n",
    "Kernel = zeros(N,N)\n",
    "for m = 1:N\n",
    "    for n = 1:N\n",
    "    Kernel[m,n] = dot(∇_xn[m],∇_xn[n])\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Kernel\n",
    "λ_t = eigen(Kernel).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Any[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for ∇ in ∇_x1\n",
    "#     println(∇)\n",
    "# end\n",
    "\n",
    "# ∇_x1[Flux.params(model)[4]]\n",
    "\n",
    "\n",
    "P = length(Flux.params(model))\n",
    "Ng = length(∇_xn)\n",
    "∇_matrix = []\n",
    "for i = 1:Ng\n",
    "    ∇_col = []\n",
    "    for j = 1:P-1\n",
    "        param_grad = ∇_xn[i][Flux.params(model)[j]]\n",
    "        push!(∇_col,param_grad)\n",
    "    end\n",
    "    push!(∇_matrix,∇_col)\n",
    "end\n",
    "\n",
    "∇_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000-element Vector{Float64}:\n",
       "  1.5325591107284708e-7\n",
       " -5.743238489230862e-6\n",
       "  4.017280843982007e-6\n",
       "  3.196689931428409e-6\n",
       "  1.827846176638559e-6\n",
       "  7.267039592306901e-8\n",
       " -4.961860668117879e-6\n",
       " -4.073514446645277e-6\n",
       "  2.33674450100807e-6\n",
       " -5.20527009939542e-6\n",
       "  ⋮\n",
       "  0.49953946471214294\n",
       "  0.5010673403739929\n",
       "  0.5017957091331482\n",
       "  0.49919593334198\n",
       "  0.4994317591190338\n",
       "  0.5015895962715149\n",
       "  0.5012345314025879\n",
       "  0.5000563859939575\n",
       "  0.5017050504684448"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to flatten\n",
    "function flatten_gradients(grad_components)\n",
    "    flat_grad = Float64[]\n",
    "    for comp in grad_components\n",
    "        # Flatten\n",
    "        append!(flat_grad, comp[:])\n",
    "    end\n",
    "    return flat_grad\n",
    "end\n",
    "\n",
    "\n",
    "flat_grads = [flatten_gradients(∇) for ∇ in ∇_matrix]\n",
    "\n",
    "flat_grads[1]\n",
    "\n",
    "# Kernel = zeros(Float64, length(flat_grads), length(flat_grads))\n",
    "# for i in 1:length(flat_grads)\n",
    "#     for j in 1:length(flat_grads)\n",
    "#         Kernel[i, j] = flat_grads[i]'*flat_grads[j]\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# Kernel\n",
    "\n",
    "# λ = eigen(Kernel).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Df (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function check_dim(x)\n",
    "    \"\"\"This function checks the appropriate  dimensions of input data\"\"\"\n",
    "    if isa(x, Matrix)\n",
    "        return size(x, 2)  # Returns the number of columns (width) of the matrix\n",
    "    elseif isa(x, Vector)\n",
    "        return 1  # Return 1 if it's a column vector\n",
    "    else\n",
    "        type = typeof(x)\n",
    "        error(\"Input data type: $type is neither a matrix or column vector\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function jac(model, x, f,param)\n",
    "    \"\"\"Gets the jacobian of a specific parameter\"\"\"\n",
    "    jaco(f) = Flux.jacobian(() -> model(x)[f],Flux.params(model))\n",
    "    return jaco(f)[Flux.params(model)[param]]\n",
    "end\n",
    "\n",
    "function Df(model, x)\n",
    "    # x: single datapoint\n",
    "    m = length(model(x))\n",
    "\n",
    "    # Total amount of θ exluding final bias\n",
    "    total_amount_of_θ = sum(length, Flux.params(model))  - length(Flux.params(model)[length(Flux.params(model))])\n",
    "\n",
    "    # Skilgreini empty jacobian matrix\n",
    "    Jacob = zeros(total_amount_of_θ,m)\n",
    "\n",
    "    for func_i = 1:m\n",
    "        current_col = Vector{Float64}(undef, 0) # Preallocate memory\n",
    "        for param_i = 1:length(Flux.params(model)) - 1\n",
    "            jac_vec = jac(model, x, func_i, param_i)[:]\n",
    "            current_col = vcat(current_col, jac_vec) # Concatenate vectors\n",
    "        end\n",
    "        for k = 1:total_amount_of_θ \n",
    "            Jacob[k, func_i] = current_col[k]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return Jacob # Þetta er Df fylkið í bilblíunni\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "J = Df(model,xn)\n",
    "\n",
    "J[:,1] == flat_grads[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
