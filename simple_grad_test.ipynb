{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "If we create the kernel matrix for the most trivial case. With a model that only has two nodes as input two notes in a single hidden layer and output is a single note.\n",
    "\n",
    "![Alt text](NN_simple.png)\n",
    "\n",
    "\n",
    "\n",
    "Let's break down the parameters for each dense layer:\n",
    "\n",
    "**First Dense layer:**\n",
    "- Input units: 2\n",
    "- Output units: 2\n",
    "- Parameters: $$[2 \\times(weights)]\\times [2\\times(input)] + 2 \\times (biases) = 6 \\textit{ parameters}$$\n",
    "\n",
    "**Second Dense layer:**\n",
    "- Input units: 2 (output units from the previous layer)\n",
    "- Output units: 1\n",
    "- Parameters: $$[1 \\times(weights)]\\times[ 2\\times(input)] + 1\\times (biases) = 3 \\textit{ parameters}$$\n",
    "\n",
    "### Kernel Matrix\n",
    "\n",
    "We want to define the function $f(x_i;\\theta)$, since we have written the listed down all the parameters it's trivial:\n",
    "\n",
    "$$f(x_i;\\theta)=W_2\\left[W_1\\times x_i + B_1\\right]+B_2$$\n",
    "\n",
    "With $\\theta$ = $\\{W_1,B_1,W_2,B_2\\}$\n",
    "\n",
    "The kernel matrix is defined as \n",
    "$$K=\\begin{bmatrix}\\sum _k \\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k} & \\ldots & \\sum _k \\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_m;\\theta)}{\\partial_k} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\sum _k \\frac{\\partial f(x_n;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_1;\\theta)}{\\partial\\theta_k} & \\ldots & \\sum _k \\frac{\\partial f(x_n;\\theta)}{\\theta_k} \\frac{\\partial f(x_m;\\theta)}{\\partial \\theta_k} \\end{bmatrix} $$\n",
    "\n",
    "We calculate the partial deriative with respect with all parameters:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_i;\\theta)}{\\partial \\theta_k}$$ \n",
    "$$ \\frac{\\partial f(x_i;\\theta)}{\\partial W_1} = W_2^Tx_i^T\\text{ ,  } \\frac{\\partial f(x_i;\\theta)}{\\partial B_1} = W_2^T \\text{ ,  }\\frac{\\partial f(x_i;\\theta)}{\\partial W_2} = [W_1x_i+B_1]^T \\text{ ,  }\\frac{\\partial f(x_i;\\theta)}{\\partial B_2} = 1\n",
    "$$\n",
    "\n",
    "With that we calculate the first entry of the kernel matrix where\n",
    "$$ x_1 = \\begin{bmatrix}a  \\\\ b \\end{bmatrix} \\text{ and } x_2 = \\begin{bmatrix}c  \\\\ d \\end{bmatrix}$$\n",
    "\n",
    "$$ K_{1,1} = \\sum_k \\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}$$ \n",
    "$$ =  W_2^T\\begin{bmatrix}a & b\\end{bmatrix}+ W$$\n",
    "Where\n",
    "$$W_1 \\in\\R^{2\\times 2}\\text{ , } W2\\in\\R^{1\\times 2}\\text{ , }B_1 \\in\\R^{1\\times 2}\\text{ , }B_2\\in\\R^{1\\times 1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: cannot broadcast array to have fewer non-singleton dimensions",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: cannot broadcast array to have fewer non-singleton dimensions\n",
      "\n",
      "Stacktrace:\n",
      " [1] check_broadcast_shape\n",
      "   @ .\\broadcast.jl:548 [inlined]\n",
      " [2] check_broadcast_shape\n",
      "   @ .\\broadcast.jl:554 [inlined]\n",
      " [3] check_broadcast_axes\n",
      "   @ .\\broadcast.jl:556 [inlined]\n",
      " [4] instantiate\n",
      "   @ .\\broadcast.jl:297 [inlined]\n",
      " [5] materialize!\n",
      "   @ .\\broadcast.jl:884 [inlined]\n",
      " [6] materialize!(dest::Vector{Float32}, bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(identity), Tuple{Matrix{Int64}}})\n",
      "   @ Base.Broadcast .\\broadcast.jl:881\n",
      " [7] top-level scope\n",
      "   @ c:\\Programming\\Github\\UROP\\simple_grad_test.ipynb:43"
     ]
    }
   ],
   "source": [
    "using Flux, LinearAlgebra\n",
    "\n",
    "x1 = [1; 2]\n",
    "x2 = [3; 4]\n",
    "\n",
    "x1 = Float32.(x1)\n",
    "x2 = Float32.(x2)\n",
    "\n",
    "X = hcat(x1,x2)\n",
    "\n",
    "model = Chain(\n",
    "    Dense(2 => 2),\n",
    "    Dense(2 => 1)\n",
    ")\n",
    "\n",
    "n = size(X)[2]\n",
    "\n",
    "\n",
    "gs_x = []\n",
    "\n",
    "for i in 1:n\n",
    "    gs = gradient(() -> model(X)[i], Flux.params(model))\n",
    "    push!(gs_x, gs)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# w1 =  model.layers[1].weight\n",
    "# w2 = model.layers[2].weight\n",
    "# b1 = model.layers[1].bias\n",
    "# b2 = model.layers[2].bias\n",
    "# @show gs_x\n",
    "\n",
    "\n",
    "# for i in 1:length(gs_x)\n",
    "#     g1_xi = gs\n",
    "\n",
    "\n",
    "gs_x1 = Flux.gradient(() -> model(x1)[1], Flux.params(model))\n",
    "\n",
    "\n",
    "Flux.params(model)[1] .= ones(2,2)\n",
    "Flux.params(model)[2] .= [1 1]\n",
    "Flux.params(model)[3] .= [1 1]\n",
    "Flux.params(model)[4] .= 1\n",
    "\n",
    "# # @show model(x1)\n",
    "g1_x1 = gs_x1[Flux.params(model)[1]] # W1 x1\n",
    "g2_x1 = gs_x1[Flux.params(model)[2]] # B1 x1\n",
    "g3_x1 = gs_x1[Flux.params(model)[3]] # W2 X1\n",
    "g4_x1 = gs_x1[Flux.params(model)[4]] # B2 x1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K_11 = dot(g1_x1,g1_x1)+dot(g2_x1,g2_x1)+dot(g2_x1,g2_x1)+dot(g3_x1,g3_x1)+dot(g4_x1,g4_x1)\n",
    "\n",
    "\n",
    "\n",
    "# δw1_x1 = w2_x1'*x1'\n",
    "# δb1_x1 = w2_x1'\n",
    "# δw2_x1 = w1_x1*x1+b1_x1; δw2_x1'\n",
    "# δb2_x1 = 1\n",
    "\n",
    "# Extracting gradients using the parameter names directly\n",
    "# δw1_x1 = gs_x1[model.layers[1].weight]\n",
    "# δb1_x1 = gs_x1[model.layers[1].bias]\n",
    "# δw2_x1 = gs_x1[model.layers[2].weight]\n",
    "# δb2_x1 = gs_x1[model.layers[2].bias]\n",
    "\n",
    "# Done by hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# display(δw1_x1)\n",
    "# display(δb1_x1)\n",
    "# display(δw2_x1)\n",
    "# display(δb2_x1)\n",
    "\n",
    "# display(g1_x1)\n",
    "# display(g2_x1)\n",
    "# display(g3_x1)\n",
    "# display(g4_x1)\n",
    "\n",
    "# gs_x1 = Flux.gradient(() -> model(x2)[1],Flux.params(model))\n",
    "\n",
    "# Extracting gradients using the parameter names directly\n",
    "# δw1_x1 = gs_x1[model.layers[1].weight]\n",
    "# δb1_x1 = gs_x1[model.layers[1].bias]\n",
    "# δw2_x1 = gs_x1[model.layers[2].weight]\n",
    "# δb2_x1 = gs_x1[model.layers[2].bias]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gs_x2 = Flux.gradient(() -> model(x2)[1],Flux.params(model))\n",
    "\n",
    "# g_3 = gs_x2[Flux.params(model)[1]] # W1 x2\n",
    "# g_4 = gs_x2[Flux.params(model)[2]] # B1 x2\n",
    "\n",
    "# K_11 = dot(g_1,g_1)+dot(g_2,g_2) # This is the whole some over the dot-product\n",
    "# K_12 = dot(g_1,g_3)+dot(g_2,g_4)\n",
    "# K_21 = dot(g_3,g_1)+dot(g_2,g_2)\n",
    "# K_22 = dot(g_3,g_3)+dot(g_4,g_4)\n",
    "\n",
    "# K_1 = hcat(K_11,K_12)\n",
    "# K_2 = hcat(K_21,K_22)\n",
    "\n",
    "# K = vcat(K_1,K_2)\n",
    "\n",
    "# eig_info = eigen(K)\n",
    "# eig_vals = eig_info.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.229333f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using MLDatasets\n",
    "using LinearAlgebra\n",
    "\n",
    "model = Chain(  Dense(2 => 2), Dense(2 => 1)) # W_2[1x2](W_1[2x2]x[2,1]+b_1[2x1])+b_2[1]\n",
    "\n",
    "x=Float32[0.5852378, 0.62436277] # random datapoint\n",
    "\n",
    "W1 = Flux.params(model)[1]  # W_1\n",
    "b1 = Flux.params(model)[2]  # b_1\n",
    "\n",
    "W1 .= ones(2,2)  #  Hér má setja eitthvað \"fixed\" fylki, breyti gildum í W1\n",
    "b1 .= [1,1]\n",
    "\n",
    "W2 = Flux.params(model)[3]  # W_1\n",
    "b2 = Flux.params(model)[4]  # b_1\n",
    "\n",
    "W2 .= ones(1,2)\n",
    "b2 .= 1\n",
    "\n",
    "y=model(x)\n",
    "\n",
    "\n",
    "gs=Flux.gradient(() -> model(x)[1],Flux.params(model))   # Reikna allar hlutaafleiður\n",
    "all_params, r = Flux.destructure(model)\n",
    "\n",
    "\n",
    "# g1_x1 = gs_x1[Flux.params(model)[1]] # W1 x1\n",
    "# g2_x1 = gs_x1[Flux.params(model)[2]] # B1 x1\n",
    "# g3_x1 = gs_x1[Flux.params(model)[3]] # W2 X1\n",
    "# g4_x1 = gs_x1[Flux.params(model)[4]] # B2 x1\n",
    "\n",
    "\n",
    "gs1_x1_1 = gs[W1]; gs1_x1 = vec(gs1_x1_1)\n",
    "gs2_x1 = gs[b1]\n",
    "gs3_x1_1 = gs[W2]; gs3_x1 = vec(gs3_x1_1)\n",
    "gs4_x1 = gs[b2]\n",
    "\n",
    "# \n",
    "K_11 = gs1_x1'*gs1_x1+gs2_x1'*gs2_x1+gs3_x1'*gs3_x1+gs4_x1'*gs4_x1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs_x = Any[Grads(...), Grads(...)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Any}:\n",
       " Grads(...)\n",
       " Grads(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux, LinearAlgebra\n",
    "\n",
    "x1 = [1; 2]\n",
    "x2 = [3; 4]\n",
    "\n",
    "x1 = Float32.(x1)\n",
    "x2 = Float32.(x2)\n",
    "\n",
    "X = hcat(x1,x2)\n",
    "\n",
    "model = Chain(\n",
    "    Dense(2 => 2),\n",
    "    Dense(2 => 1)\n",
    ")\n",
    "\n",
    "n = size(X)[2]\n",
    "\n",
    "\n",
    "gs_x = []\n",
    "\n",
    "for i in 1:n\n",
    "    gs = gradient(() -> model(X)[i], Flux.params(model))\n",
    "    push!(gs_x, gs)\n",
    "end\n",
    "\n",
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(2 => 2),                        \u001b[90m# 6 parameters\u001b[39m\n",
       "  Dense(2 => 1),                        \u001b[90m# 3 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m9 parameters, 292 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(2 => 2),\n",
    "    Dense(2 => 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of n-th model\n",
    "\n",
    "The model function for a model with h many hidden layers. With\n",
    "$n = h+1\\textit{ , } f_0 = x_i$:\n",
    "$$f(x_i;\\theta)_n = W_n f_{n-1} + B_n$$\n",
    "\n",
    "for the first two hidden layers we show the derivative with respect to all parameters $\\theta$\n",
    "\n",
    "##### For h = 0, n = 1\n",
    "$f(x_i;\\theta)_1 = W_1x_i + B_1 $\n",
    "$$\\frac{\\partial f(x_i;\\theta)_1}{\\theta_k} \\text{ ;  } \\frac{\\partial f(x_i;\\theta)_1}{\\partial W_1}= x_i^T \\text{ ,  }\\frac{\\partial f(x_i;\\theta)_1}{\\partial B_1} = 1 $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
