{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "If we create the kernel matrix for the most trivial case. With a model that only has two nodes as input two notes in a single hidden layer and output is a single note.\n",
    "\n",
    "![Alt text](NN_simple.png)\n",
    "\n",
    "\n",
    "\n",
    "Let's break down the parameters for each dense layer:\n",
    "\n",
    "**First Dense layer:**\n",
    "- Input units: 2\n",
    "- Output units: 2\n",
    "- Parameters: $$[2 \\times(weights)]\\times [2\\times(input)] + 2 \\times (biases) = 6 \\textit{ parameters}$$\n",
    "\n",
    "**Second Dense layer:**\n",
    "- Input units: 2 (output units from the previous layer)\n",
    "- Output units: 1\n",
    "- Parameters: $$[1 \\times(weights)]\\times[ 2\\times(input)] + 1\\times (biases) = 3 \\textit{ parameters}$$\n",
    "\n",
    "### Kernel Matrix\n",
    "\n",
    "We want to define the function $f(x_i;\\theta)$, since we have written the listed down all the parameters it's trivial:\n",
    "\n",
    "$$f(x_i;\\theta)=W_2\\left[W_1\\times x_i + B_1\\right]+B_2$$\n",
    "\n",
    "The kernel matrix is defined as \n",
    "$$K=\\begin{bmatrix}\\sum _k \\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k} & \\ldots & \\sum _k \\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_m;\\theta)}{\\partial_k} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\sum _k \\frac{\\partial f(x_n;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_1;\\theta)}{\\partial\\theta_k} & \\ldots & \\sum _k \\frac{\\partial f(x_n;\\theta)}{\\theta_k} \\frac{\\partial f(x_m;\\theta)}{\\partial \\theta_k} \\end{bmatrix} $$\n",
    "\n",
    "We calculate the partial deriative with respect with all parameters:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_i;\\theta)}{\\partial \\theta_k}$$ \n",
    "$$ \\frac{\\partial f(x_i;\\theta)}{\\partial W_1} = W_2^Tx_i^T\\text{ ,  } \\frac{\\partial f(x_i;\\theta)}{\\partial B_1} = W_2^T \\text{ ,  }\\frac{\\partial f(x_i;\\theta)}{\\partial W_2} = [W_1x_i+B_1]^T \\text{ ,  }\\frac{\\partial f(x_i;\\theta)}{\\partial B_2} = 1\n",
    "$$\n",
    "\n",
    "With that we calculate the first entry of the kernel matrix where\n",
    "$$ x_1 = \\begin{bmatrix}a  \\\\ b \\end{bmatrix} \\text{ and } x_2 = \\begin{bmatrix}c  \\\\ d \\end{bmatrix}$$\n",
    "\n",
    "$$ K_{1,1} = \\sum_k \\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}\\frac{\\partial f(x_1;\\theta)}{\\partial \\theta_k}$$ \n",
    "$$ =  $$\n",
    "Where\n",
    "$$W_1 \\in\\R^{2\\times 2}\\text{ , } W2\\in\\R^{1\\times 2}\\text{ , }B_1 \\in\\R^{1\\times 2}\\text{ , }B_2\\in\\R^{1\\times 1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.96681f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux, LinearAlgebra\n",
    "\n",
    "x1 = [1; 2]\n",
    "x2 = [3; 4]\n",
    "\n",
    "x1 = Float32.(x1)\n",
    "x2 = Float32.(x2)\n",
    "\n",
    "X = hcat(x1,x2)\n",
    "\n",
    "model = Chain(\n",
    "    Dense(2 => 2),\n",
    "    Dense(2 => 1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# w1 =  model.layers[1].weight\n",
    "# w2 = model.layers[2].weight\n",
    "# b1 = model.layers[1].bias\n",
    "# b2 = model.layers[2].bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in 1:\n",
    "gs_x1 = Flux.gradient(() -> model(X)[2],Flux.params(model))\n",
    "# gs_x2 = Flux.gradient(() -> model(x2)[1], Flux)\n",
    "\n",
    "# # @show model(x1)\n",
    "g1_x1 = gs_x1[Flux.params(model)[1]] # W1 x1\n",
    "g2_x1 = gs_x1[Flux.params(model)[2]] # B1 x1\n",
    "g3_x1 = gs_x1[Flux.params(model)[3]] # W2 X1\n",
    "g4_x1 = gs_x1[Flux.params(model)[4]] # B2 x1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K_11 = dot(g1_x1,g1_x1)+dot(g2_x1,g2_x1)+dot(g2_x1,g2_x1)+dot(g3_x1,g3_x1)+dot(g4_x1,g4_x1)\n",
    "\n",
    "\n",
    "\n",
    "# δw1_x1 = w2_x1'*x1'\n",
    "# δb1_x1 = w2_x1'\n",
    "# δw2_x1 = w1_x1*x1+b1_x1; δw2_x1'\n",
    "# δb2_x1 = 1\n",
    "\n",
    "# Extracting gradients using the parameter names directly\n",
    "# δw1_x1 = gs_x1[model.layers[1].weight]\n",
    "# δb1_x1 = gs_x1[model.layers[1].bias]\n",
    "# δw2_x1 = gs_x1[model.layers[2].weight]\n",
    "# δb2_x1 = gs_x1[model.layers[2].bias]\n",
    "\n",
    "# Done by hand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# display(δw1_x1)\n",
    "# display(δb1_x1)\n",
    "# display(δw2_x1)\n",
    "# display(δb2_x1)\n",
    "\n",
    "# display(g1_x1)\n",
    "# display(g2_x1)\n",
    "# display(g3_x1)\n",
    "# display(g4_x1)\n",
    "\n",
    "# gs_x1 = Flux.gradient(() -> model(x2)[1],Flux.params(model))\n",
    "\n",
    "# Extracting gradients using the parameter names directly\n",
    "# δw1_x1 = gs_x1[model.layers[1].weight]\n",
    "# δb1_x1 = gs_x1[model.layers[1].bias]\n",
    "# δw2_x1 = gs_x1[model.layers[2].weight]\n",
    "# δb2_x1 = gs_x1[model.layers[2].bias]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gs_x2 = Flux.gradient(() -> model(x2)[1],Flux.params(model))\n",
    "\n",
    "# g_3 = gs_x2[Flux.params(model)[1]] # W1 x2\n",
    "# g_4 = gs_x2[Flux.params(model)[2]] # B1 x2\n",
    "\n",
    "# K_11 = dot(g_1,g_1)+dot(g_2,g_2) # This is the whole some over the dot-product\n",
    "# K_12 = dot(g_1,g_3)+dot(g_2,g_4)\n",
    "# K_21 = dot(g_3,g_1)+dot(g_2,g_2)\n",
    "# K_22 = dot(g_3,g_3)+dot(g_4,g_4)\n",
    "\n",
    "# K_1 = hcat(K_11,K_12)\n",
    "# K_2 = hcat(K_21,K_22)\n",
    "\n",
    "# K = vcat(K_1,K_2)\n",
    "\n",
    "# eig_info = eigen(K)\n",
    "# eig_vals = eig_info.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `column` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `column` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Programming\\Github\\UROP\\simple_grad_test.ipynb:1"
     ]
    }
   ],
   "source": [
    "size(zeros(2,2))[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
